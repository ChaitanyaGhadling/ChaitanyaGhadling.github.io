# section information
section:
  name: Experiences
  id: experiences
  enable: true
  weight: 3
  showOnNavbar: true
  # Can optionally hide the title in sections
  # hideTitle: true

# Your experiences
experiences:
  - company:
      name: Frontier Communications.
      url: "https://www.frontier.com"
      location: Dallas, TX
      # company overview
      #overview: Example Co. is a widely recognized company for cloud-native development. It builds tools for Kubernetes.
    positions:
      - designation: Data Engineer
        start: July 2023
        end: Nov 2024
        # don't provide end date if you are currently working there. It will be replaced by "Present"
        # end: Dec 2020
        # give some points about what was your responsibilities at the company.
        responsibilities:
          - Designed, implemented, and maintained an ETL pipeline integrating **NetSuite** ERP data with **Apache Kafka, Airflow, Spark, Docker, and AWS** services for seamless data flow and processing.
          - Configured **Kafka** topics and consumers for real-time ingestion of transactional data from NetSuite, reducing data latency by **40%** and ensuring robust pipelines.
          - Utilized **AWS S3** as a data lake to store raw and processed data, ensuring scalability, durability, and cost-effectiveness.
          - Leveraged **AWS Glue** for cataloging NetSuite metadata and orchestrating ETL jobs that transform ERP data into analytics-ready formats.
          - Incorporated unstructured data, such as logs or external documents, into the pipeline by leveraging AWS S3 as a storage layer and processing it with **Apache Spark**, transforming it into actionable insights alongside structured NetSuite data.
          - Developed optimized data models for NetSuite-generated structured data, stored in **AWS RDS** and Amazon **Redshift** to support advanced reporting and BI use cases.
          - Implemented monitoring and logging solutions using **AWS CloudWatch and AWS CloudTrail** to track pipeline performance metrics, identify bottlenecks, and optimize data processing workflows resulting in a 25% improvement in pipeline efficiency.  
          - Collaborated with data analysts, developers, and NetSuite ERP administrators to gather business requirements, implement data solutions, and ensure alignment with organizational goals.
  - company:
      name: New Jersey Institute Of Technology.
      url: "https://www.njit.edu"
      location: Newark, NJ
      # company overview
      #overview: Example Co. is a widely recognized company for cloud-native development. It builds tools for Kubernetes.
    positions:
      - designation: AI/ML Engineer
        start: April 2023
        end: July 2024
        # don't provide end date if you are currently working there. It will be replaced by "Present"
        # end: Dec 2020
        # give some points about what was your responsibilities at the company.
        responsibilities:
          - Developed a novel image rendering module for a 3D video conferencing tool, utilizing various Deep Learning tools like **Pytorch and Tensorflow** for efficient and scalable model training in a distributed environment.
          - Implemented advanced Deep Learning techniques like **GANs, U-Net, Encoder/Decoder** framework to achieve high-quality results in the inpainting task.
          - Created a custom dataset and dataLoader from scratch tailored to model requirements for better results.
          - Collaborated closely with PhD students to research state of art algorithms and seamlessly integrate novel methodologies into existing software architecture, ensuring smooth functionality and enhanced performance.
  - company:
      name: GCB Services.
      url: "http://gcbservices.com/"
      location: McLean, VA
      #overview: Telecommunication company providing optimal engineering and business solutions for the wireless telecom industry.
    positions:
      - designation: Data Engineer
        start: June 2022
        end: August 2022
        responsibilities:
          - Automated daily tasks of the company by developing multiple standalone applications, deployed on **AWS S3**, using **Selenium** and Xlsxwriter, resulting in a 70% reduction in routine workload.
          - Pioneered the company's transition of data storage and analysis from Excel/VBA to **SQL** by designing a centralized database and deploying it on **AWS RDS** Server from scratch.
          - Extracted valuable insights by performing extensive data analysis from diverse sources, for performance tracking and enabling more informed client billing decisions, resulting in a remarkable 23% increase in company revenue.
          - Enhanced project readability by thoroughly documenting existing code, providing clear explanations, and improving overall code maintainability.
          - Streamlined the visualization of critical KPIs by developing a web application using the **Streamlit** framework, deployed on Heroku facilitating informed decision-making.

  - company:
      name: New Jersey Institute Of Technology.
      url: "https://www.njit.edu"
      location: Newark, NJ
      #overview: PreExample Co. is a gateway company to enter into Example co. So, nothing special here.
    positions:
      - designation: Research Assistant
        start: March 2022
        end: May 2022
        responsibilities:
          - Collaborated on a project focused on mapping wetland inundation through remote sensing techniques.
          - Engaged In Data pre-processing phase, labeling over 10,000 image segments using **ArcGIS**.
          - Explored different Deep Learning models and assisted in hyper-parameter tuning of different **U-Net** models.

  - company:
      name: PiSyst Pvt Ltd.
      url: "https://www.pi-sy.com/"
      location: India
      #overview: Software Development Company providing one-stop IT solution.
    positions:
      - designation: Data Analyst Intern
        start: April 2020
        end: September 2020
        responsibilities:
          - Established **ETL** workflows using **Informatica** to extract, transform, and load data from various sources into our data warehouse, resulting in a 30% reduction in data processing time.
          - Devised ad-hoc analysis using **SQL, Python** and executed **A/B tests** resulting in improved business decision-making.
          - Exhibited proficiency in executing complex **SQL queries** and effectively utilizing existing tables to create views based on specific requirements.
          - Designed custom visualizations in **PowerBI** to display complex data in an interactive manner.
